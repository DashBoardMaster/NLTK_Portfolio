{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below imports nltk and downloads all the packages that we will be using henceforth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pecan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pecan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pecan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\pecan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.book import *\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.text import *\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I learned about how the tokens method tokenizes based on word and sentence. I learned how text objects have many methods wrapped within them that enable you to use them in novel and innovative ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text1[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance(\"sea\",80,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works by counting every instance of the specified token within the text and giving you the count. This works the same as pythons count method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presidents = ['obama','obama','biden', 'trump']\n",
    "print(presidents.count('obama'))\n",
    "\n",
    "tokener = [t.lower() for t in text1]\n",
    "\n",
    "setter = set(tokener)\n",
    "\n",
    "\n",
    "wordLemmat = nltk.WordNetLemmatizer()\n",
    "\n",
    "lemmatLister = [wordLemmat.lemmatize(i) for i in tokener]\n",
    "\n",
    "\n",
    "lemmaSetter = set(lemmatLister)  # creating a new list that removes duplicates\n",
    "\n",
    "removePunct = [\",\", \"?\", \".\", \":\", \"'\", \"[\", \"]\",\n",
    "               \"{\", \"}\", \"--\", \"#\", \"...\", \"!\", \"-\", \"'!\", \"...\", \";\", \",\"]\n",
    "removeStop = stopwords.words(\"english\") + [\"wa\"]\n",
    "\n",
    "removedList = [\n",
    "    storedvalue for storedvalue in lemmatLister if storedvalue not in removePunct+removeStop]\n",
    "\n",
    "variable = {x: removedList.count(x) for x in lemmaSetter}\n",
    "\n",
    "sorted_counts = sorted(variable.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the first 5 sentences of Moby Dick by Herman Melville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Call', 'me', 'Ishmael', '.', 'Some', 'years', 'ago', '-', 'never', 'mind']\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"Call me Ishmael. Some years ago - never mind how long precisely having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world. It is a way I have of driving off the spleen and regulating the circulation. Whenever I find myself growing grim about the mouth whenever it is a damp, drizzly November in my soul whenever I find myself involuntarily pausing before coffin warehouses, and bringing up the rear of every funeral I meet and especially whenever my hypos get such an upper hand of me, that it requires a strong moral principle to prevent me from deliberately stepping into the street, and methodically knocking people’s hats off - then, I account it high time to get to sea as soon as I can. This is my substitute for pistol and ball.\"\n",
    "\n",
    "tokens = word_tokenize(raw_text)\n",
    "print(tokens[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Call me Ishmael.', 'Some years ago - never mind how long precisely having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world.', 'It is a way I have of driving off the spleen and regulating the circulation.', 'Whenever I find myself growing grim about the mouth whenever it is a damp, drizzly November in my soul whenever I find myself involuntarily pausing before coffin warehouses, and bringing up the rear of every funeral I meet and especially whenever my hypos get such an upper hand of me, that it requires a strong moral principle to prevent me from deliberately stepping into the street, and methodically knocking people’s hats off - then, I account it high time to get to sea as soon as I can.', 'This is my substitute for pistol and ball.']\n"
     ]
    }
   ],
   "source": [
    "sent_tokens = sent_tokenize(raw_text)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['call', 'me', 'ishmael', '.', 'some', 'year', 'ago', '-', 'never', 'mind', 'how', 'long', 'precis', 'have', 'littl', 'or', 'no', 'money', 'in', 'my', 'purs', ',', 'and', 'noth', 'particular', 'to', 'interest', 'me', 'on', 'shore', ',', 'i', 'thought', 'i', 'would', 'sail', 'about', 'a', 'littl', 'and', 'see', 'the', 'wateri', 'part', 'of', 'the', 'world', '.', 'it', 'is', 'a', 'way', 'i', 'have', 'of', 'drive', 'off', 'the', 'spleen', 'and', 'regul', 'the', 'circul', '.', 'whenev', 'i', 'find', 'myself', 'grow', 'grim', 'about', 'the', 'mouth', 'whenev', 'it', 'is', 'a', 'damp', ',', 'drizzli', 'novemb', 'in', 'my', 'soul', 'whenev', 'i', 'find', 'myself', 'involuntarili', 'paus', 'befor', 'coffin', 'warehous', ',', 'and', 'bring', 'up', 'the', 'rear', 'of', 'everi', 'funer', 'i', 'meet', 'and', 'especi', 'whenev', 'my', 'hypo', 'get', 'such', 'an', 'upper', 'hand', 'of', 'me', ',', 'that', 'it', 'requir', 'a', 'strong', 'moral', 'principl', 'to', 'prevent', 'me', 'from', 'deliber', 'step', 'into', 'the', 'street', ',', 'and', 'method', 'knock', 'peopl', '’', 's', 'hat', 'off', '-', 'then', ',', 'i', 'account', 'it', 'high', 'time', 'to', 'get', 'to', 'sea', 'as', 'soon', 'as', 'i', 'can', '.', 'thi', 'is', 'my', 'substitut', 'for', 'pistol', 'and', 'ball', '.']\n"
     ]
    }
   ],
   "source": [
    "portStem = PorterStemmer()\n",
    "stem_list = [portStem.stem(x) for x in tokens]\n",
    "print(stem_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precis-precisely\n",
    "hav-having\n",
    "littl-little\n",
    "purs-purse\n",
    "noth-nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Call', 'me', 'Ishmael', '.', 'Some', 'year', 'ago', '-', 'never', 'mind', 'how', 'long', 'precisely', 'having', 'little', 'or', 'no', 'money', 'in', 'my', 'purse', ',', 'and', 'nothing', 'particular', 'to', 'interest', 'me', 'on', 'shore', ',', 'I', 'thought', 'I', 'would', 'sail', 'about', 'a', 'little', 'and', 'see', 'the', 'watery', 'part', 'of', 'the', 'world', '.', 'It', 'is', 'a', 'way', 'I', 'have', 'of', 'driving', 'off', 'the', 'spleen', 'and', 'regulating', 'the', 'circulation', '.', 'Whenever', 'I', 'find', 'myself', 'growing', 'grim', 'about', 'the', 'mouth', 'whenever', 'it', 'is', 'a', 'damp', ',', 'drizzly', 'November', 'in', 'my', 'soul', 'whenever', 'I', 'find', 'myself', 'involuntarily', 'pausing', 'before', 'coffin', 'warehouse', ',', 'and', 'bringing', 'up', 'the', 'rear', 'of', 'every', 'funeral', 'I', 'meet', 'and', 'especially', 'whenever', 'my', 'hypo', 'get', 'such', 'an', 'upper', 'hand', 'of', 'me', ',', 'that', 'it', 'requires', 'a', 'strong', 'moral', 'principle', 'to', 'prevent', 'me', 'from', 'deliberately', 'stepping', 'into', 'the', 'street', ',', 'and', 'methodically', 'knocking', 'people', '’', 's', 'hat', 'off', '-', 'then', ',', 'I', 'account', 'it', 'high', 'time', 'to', 'get', 'to', 'sea', 'a', 'soon', 'a', 'I', 'can', '.', 'This', 'is', 'my', 'substitute', 'for', 'pistol', 'and', 'ball', '.']\n"
     ]
    }
   ],
   "source": [
    "netLemmatizer = nltk.WordNetLemmatizer()\n",
    "lemmatizedList = [netLemmatizer.lemmatize(i) for i in tokens]\n",
    "print(lemmatizedList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. The NLTK library has quite extensive functionality that gives a researcher or developer the tools necessary to parse and understand text. These tools can be used to build tools that can perform many applications related to text comprehension, generation, etc. I think the code in the NLTK library is quite readable and effecient , it is suitable for building large applications. I may use the NLTK library to build text comprehension software that could summarize text without losing the inherent meaning within it. This would be quite useful for many applications such as news summarizing so that you don't have to spend as much time in the morning reading the news."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14f32a1ef65e954a537a4d8ff23e1d99bb660fdc54f5c622bd6001605325d797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
